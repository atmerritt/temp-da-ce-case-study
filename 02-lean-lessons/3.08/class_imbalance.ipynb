{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pair-coding lab: Introducing class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this **lab**, you'll work in pairs or small groups to explore the issue of class imbalance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we'll start by loading relevant libraries and setting up our figure aesthetics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import seaborn as sns\n",
    "import cmocean\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "\n",
    "viz_style = {\n",
    "    'font.family': 'sans-serif',\n",
    "    'font.size':11,\n",
    "    'axes.titlesize':'large',\n",
    "    'axes.labelsize':'medium',\n",
    "    'xtick.labelsize':'small',\n",
    "    'ytick.labelsize':'small',\n",
    "    'text.color':'#5B5654',\n",
    "    'axes.labelcolor':'#5B5654',\n",
    "    'xtick.color':'#5B5654',\n",
    "    'ytick.color':'#5B5654',\n",
    "    'axes.edgecolor':'#5B5654',\n",
    "    'xtick.top':False,\n",
    "    'ytick.right':False,\n",
    "    'axes.spines.top':False,\n",
    "    'axes.spines.right':False,\n",
    "    'axes.grid':False,\n",
    "    'boxplot.showfliers':False,\n",
    "    'boxplot.patchartist':True\n",
    "}\n",
    "\n",
    "plt.style.use(viz_style)\n",
    "\n",
    "case_dir = '/path/to/materials'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanced classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of comparison, we'll start out with a balanced dataset.\n",
    "\n",
    "`Sklearn` has a very cool function, `make_classification`, that allows us to generate our own datasets with a lot of flexibility. We'll use it to compare balanced and imbalanced datasets, and to explore how the properties of an imbalanced dataset can affect the difficulty of a project.\n",
    "\n",
    "Check out the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) for `make_classification`, and in particular pay attention to the many aspects that we can control with various parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a balanced dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our balanced dataset will contain 5000 samples, split evenly into 3 classes. We'll make 5 features, and to be kind to ourselves we'll allow 4 of them to be informative/useful (the 5th will be redundant). We will also impose the simplifying constraint that the distribution of feature space covered by each class only has *one* cluster. Finally, we'll keep the noise in the dataset low, and only allow 1% of the classes to be assigned randomly.\n",
    "\n",
    "---\n",
    "\n",
    "Run the cells below to create the dataset and convert the results to a pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "X_balanced, y_balanced = make_classification(n_classes=3, weights=None, n_samples=5000, \n",
    "                                             n_features=5, n_redundant=1, n_informative=4,\n",
    "                                             n_clusters_per_class=1, random_state=0, flip_y=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the dataset to a pandas dataframe\n",
    "df_balanced = pd.concat([pd.DataFrame(data=X_balanced), \n",
    "                         pd.DataFrame(data=y_balanced, columns=['label'])], \n",
    "                        axis=1)\n",
    "df_balanced.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what our data look like via a pair grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.PairGrid(df_balanced, diag_sharey=False, corner=True, hue='label', palette='crest_r',\n",
    "                 height=1.7, aspect=1)\n",
    "g.map_lower(sns.scatterplot)\n",
    "g.map_diag(sns.kdeplot)\n",
    "g.add_legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do the usual train-test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_balanced[df_balanced.columns[:-1]], \n",
    "                                                    df_balanced['label'], \n",
    "                                                    random_state=0, test_size=0.25, \n",
    "                                                    stratify=df_balanced['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can set up our logistic regression model!\n",
    "\n",
    "**Note:** you can choose whether to use OVR or multinomial here! The following cell uses the default (which is multinomial), but it's up to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train)\n",
    "X_test_sc = scaler.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression(random_state=0).fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to do a more robust evaluation of the model than accuracy alone, so you can use this convenience function (below) to compactly summarize your results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "def summarize_scores(y_true, y_pred, settype='Training'):\n",
    "    print('\\n--- {} SET ---'.format(settype.upper()))\n",
    "    print('Accuracy:', accuracy_score(y_true, y_pred))\n",
    "    print('Precision:', precision_score(y_true, y_pred, average='macro'))\n",
    "    print('Recall:', recall_score(y_true, y_pred, average='macro'))\n",
    "    print('F1:', f1_score(y_true, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how your model did:\n",
    "summarize_scores(y_train, clf.predict(X_train_sc), settype='Training')\n",
    "summarize_scores(y_test, clf.predict(X_test_sc), settype='Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, as before, we have the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "heatmap_train = sns.heatmap(confusion_matrix(y_train, clf.predict(X_train_sc), normalize=\"true\"), \n",
    "                            annot=True, fmt='.2%', cmap=\"PuBu\");\n",
    "heatmap_train.set_xlabel('Predicted label')\n",
    "heatmap_train.set_ylabel('True label')\n",
    "heatmap_train.set_title('Balanced Training set');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_test = sns.heatmap(confusion_matrix(y_test, clf.predict(X_test_sc), normalize=\"true\"), \n",
    "                           annot=True, fmt='.2%', cmap=\"PuBu\")\n",
    "heatmap_test.set_xlabel('Predicted label')\n",
    "heatmap_test.set_ylabel('True label')\n",
    "heatmap_test.set_title('Balanced Test set');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can do better with optimized parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'C':10**np.linspace(-2,2, 11)}\n",
    "gs = GridSearchCV(LogisticRegression(multi_class='multinomial', random_state=0), \n",
    "                  parameters, scoring='recall_macro').fit(X_train_sc, y_train)\n",
    "print('Best params:', gs.best_params_)\n",
    "\n",
    "summarize_scores(y_train, gs.predict(X_train_sc), settype='Training')\n",
    "summarize_scores(y_test, gs.predict(X_test_sc), settype='Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the scores are uniformly high!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imbalanced classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this next section, we'll look at a dataset with class imbalance and see how this is different from the balanced class. You'll start with a pre-defined imbalanced dataset, but later you'll make your own and explore!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build an imbalanced dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the balanced dataset, our **imbalanced** dataset has 5000 samples and 3 classes. We still have 5 features (4 of which are informative), one central cluster per class, and randomly assign 1% of the sample classes.\n",
    "\n",
    "However, this time, the samples are *not* split evenly between the 3 classes. Instead we have:    \n",
    ">Class 0: 90%  \n",
    "Class 1: 5%  \n",
    "Class 2: 5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "from sklearn.datasets import make_classification\n",
    "X_imbalanced, y_imbalanced = make_classification(n_classes=3, weights=[0.9, 0.05, 0.05], n_samples=5000,\n",
    "                                                 n_features=5, n_redundant=1, n_informative=4,\n",
    "                                                 n_clusters_per_class=1, random_state=0, flip_y=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to pandas dataframe\n",
    "df_imbalanced = pd.concat([pd.DataFrame(data=X_imbalanced), \n",
    "                         pd.DataFrame(data=y_imbalanced, columns=['label'])], \n",
    "                        axis=1)\n",
    "df_imbalanced.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a pair grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.PairGrid(df_imbalanced, diag_sharey=False, corner=True, hue='label', palette='crest_r',\n",
    "                 height=1.7, aspect=1)\n",
    "g.map_lower(sns.scatterplot)\n",
    "g.map_diag(sns.kdeplot)\n",
    "g.add_legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is this different from the first dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_imbalanced[df_imbalanced.columns[:-1]], \n",
    "                                                    df_imbalanced['label'], \n",
    "                                                    random_state=0, test_size=0.25, \n",
    "                                                    stratify=df_imbalanced['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train)\n",
    "X_test_sc = scaler.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression(random_state=0).fit(X_train_sc, y_train)\n",
    "\n",
    "# see how your model did:\n",
    "summarize_scores(y_train, clf.predict(X_train_sc), settype='Training')\n",
    "summarize_scores(y_test, clf.predict(X_test_sc), settype='Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the different evaluation metric scores. Which ones are high? Which ones are low? Discuss with your partner why this might be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "\n",
    "Check out the confusion matrices for additional context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_train = sns.heatmap(confusion_matrix(y_train, clf.predict(X_train_sc), normalize=\"true\"), \n",
    "                            annot=True, fmt='.2%', cmap=\"PuBu\");\n",
    "heatmap_train.set_xlabel('Predicted label')\n",
    "heatmap_train.set_ylabel('True label')\n",
    "heatmap_train.set_title('Imbalanced Training set');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_test = sns.heatmap(confusion_matrix(y_test, clf.predict(X_test_sc), normalize=\"true\"), \n",
    "                           annot=True, fmt='.2%', cmap=\"PuBu\")\n",
    "heatmap_test.set_xlabel('Predicted label')\n",
    "heatmap_test.set_ylabel('True label')\n",
    "heatmap_test.set_title('Imbalanced Test set');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can do better via optimizing parameters, but this time, **experiment with different scoring metrics for optimization.** (Check out the `GridSearchCV` [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html), specifically the \"scoring\" parameter).\n",
    "\n",
    "Different choices of scoring metrics for your grid search will lead to different optimal parameters, which in turn will lead to different model performances. How do your results compare when you set `scoring='recall_macro'` vs `scoring='precision_macro'` vs `scoring='accuracy'` (the default) vs. `scoring='balanced_accuracy'`? \n",
    "\n",
    "Discuss with your partner why we might want to use `_macro` rather than `_micro` for imbalanced classes (see [the docs](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score) for a refresher)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'C':10**np.linspace(-2,2, 11)}\n",
    "gs = GridSearchCV(LogisticRegression(random_state=0), \n",
    "                  parameters, scoring='recall_macro').fit(X_train_sc, y_train)\n",
    "print('Best params:', gs.best_params_)\n",
    "\n",
    "# see how your model did:\n",
    "summarize_scores(y_train, gs.predict(X_train_sc), settype='Training')\n",
    "summarize_scores(y_test, gs.predict(X_test_sc), settype='Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, try balancing class weights in your logistic regression model. Weights can be specified, but the default is to weight inversely by the size of the class (so that smaller classes get larger weights). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'C':10**np.linspace(-2,2, 11)}\n",
    "gs = GridSearchCV(LogisticRegression(class_weight='balanced', random_state=0), \n",
    "                  parameters, scoring='recall_macro').fit(X_train_sc, y_train)\n",
    "print('Best params:', gs.best_params_)\n",
    "\n",
    "# see how your model did:\n",
    "summarize_scores(y_train, gs.predict(X_train_sc), settype='Training')\n",
    "summarize_scores(y_test, gs.predict(X_test_sc), settype='Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did each evaluation metric change relative to the model without class weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make your own datasets using `make_classification` to answer the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Degree of imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first example we used, the class breakdown was 90%/5%/5%. What happens as you increase or decrease the imbalance of classes in the dataset? (Note: the smaller classes do not always have to have the same weights! They just all need to sum to 100%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number and similarity of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try increasing the number of classes and/or playing around with the `class_sep` parameter to see what happens as the boundaries between classes change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noisiness of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we increase or decrease the fraction of samples that are assigned random classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number and usefulness of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try changing the number of features and the number of informative features! How does this affect the feasibility of a classification project?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Etc!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep playing with the `make_classification` parameters. How complicated can you make things? Think about how mess real-world data is. What are the biggest challenges?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "ctrl-shift-e"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "286.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
