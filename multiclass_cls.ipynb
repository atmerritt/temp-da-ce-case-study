{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guided project: Diving into multi-class classification via logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a **guided project** that we'll step through together as a class to understand multi-class logistic regression!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import some useful libraries and (optionally) set up some plot aesthetics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import seaborn as sns\n",
    "import cmocean\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "\n",
    "viz_style = {\n",
    "    'font.family': 'sans-serif',\n",
    "    'font.size':11,\n",
    "    'axes.titlesize':'large',\n",
    "    'axes.labelsize':'medium',\n",
    "    'xtick.labelsize':'small',\n",
    "    'ytick.labelsize':'small',\n",
    "    'text.color':'#5B5654',\n",
    "    'axes.labelcolor':'#5B5654',\n",
    "    'xtick.color':'#5B5654',\n",
    "    'ytick.color':'#5B5654',\n",
    "    'axes.edgecolor':'#5B5654',\n",
    "    'xtick.top':False,\n",
    "    'ytick.right':False,\n",
    "    'axes.spines.top':False,\n",
    "    'axes.spines.right':False,\n",
    "    'axes.grid':False,\n",
    "    'boxplot.showfliers':False,\n",
    "    'boxplot.patchartist':True\n",
    "}\n",
    "\n",
    "plt.style.use(viz_style)\n",
    "\n",
    "case_dir = '/path/to/materials'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're ready to go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project we'll be using the classic [Iris Dataset](https://archive.ics.uci.edu/ml/datasets/Iris/). This is a very simple dataset, with only 4 features and 3 classes corresponding to different species of Iris: Setosa, Versicolour and Virginica. The features describe the length and width of the petals and sepals.\n",
    "\n",
    "---\n",
    "\n",
    "We'll read in the data using `sklearn`'s function `load_iris`. There are a number of other convenience functions that will allow access to other datasets, and if you're interested you can check them out here: https://sklearn.org/datasets/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "data.keys() # summarize information in 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the target and feature names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our exploratory data analysis will be easier if we can work with a dataframe, so let's create one now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data= np.c_[data['data'], data['target']],\n",
    "                     columns= data['feature_names'] + ['species'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play around with the data and explore each of the 4 features! What are their distributions? How are the three classes distributed? Are any features correlated with the others? What is the relation between each feature (or combination of features) with the target variable? \n",
    "\n",
    "---\n",
    "\n",
    "Recommended visualizations:\n",
    "* Individual features: [Histogram](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.hist.html)\n",
    "* Feature vs species class: [Box plot](https://seaborn.pydata.org/generated/seaborn.boxplot.html)\n",
    "* Relations between features and other features/target: [Pair grid](https://seaborn.pydata.org/generated/seaborn.PairGrid.html#seaborn.PairGrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think about this dataset? Do you think these features are good predictors of the species of Iris? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluating models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is split our data into a training set and a test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[data.feature_names], df['species'], \n",
    "                                                    random_state=0, test_size=0.25, \n",
    "                                                    stratify=df['species'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice we're using the `stratify` parameter when we run `train_test_split` here. Check out the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html); what does this parameter do? Why might it be important here? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one-vs-rest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll load in the logistic regression model, the standard scaler for preprocessing, and a couple model evaluation metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale your features, define your model (note we are specifying the one-vs-rest method via the `multi_class='ovr'` parameter setting), and let's see how it does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train)\n",
    "X_test_sc = scaler.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression(multi_class='ovr', random_state=0).fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--- TRAINING SET ---')\n",
    "print('Accuracy:', accuracy_score(y_train, clf.predict(X_train_sc)))\n",
    "\n",
    "print('\\n--- TEST SET ---')\n",
    "print('Accuracy:', accuracy_score(y_test, clf.predict(X_test_sc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to *visually* assess the performance of a model is by making a **confusion matrix.** For *N* classes, this is an $N\\times N$ grid with the true labels on the y-axis and predicted labels on the x-axis. The value of each grid tells you how the predictions for a given class were distributed.\n",
    "\n",
    "For example, a perfect classifier would have predict class \"0\" for 100% of the true class \"0\" samples, and similarly for class \"1\" and \"2.\" We would see this as a diagonal strip of 100% in the confusion matrix, with 0% everywhere else.\n",
    "\n",
    "In reality, though, classifiers are rarely perfect. Let's see what ours looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_train = sns.heatmap(confusion_matrix(y_train, clf.predict(X_train_sc), normalize=\"true\"), \n",
    "                            annot=True, fmt='.2%', cmap=\"PuBu\");\n",
    "heatmap_train.set_xlabel('Predicted label')\n",
    "heatmap_train.set_ylabel('True label')\n",
    "heatmap_train.set_title('Training set');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the strengths and weaknesses of the model? Which classes does it have an easier or harder time with?\n",
    "\n",
    "---\n",
    "\n",
    "Make a confusion matrix for the **test set**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the differences between the training and test results? Can you explain these differences?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the one-vs-rest method produces the probabilities of belong to _each_ class for every data point. We can see these via `clf.predict_proba` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict_proba(X_test_sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might be able to do better than this though! Run a grid search to see which value of the regularization parameter `C` is best for this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'C':10**np.linspace(-2,2, 11)} # feel free to change this\n",
    "gs = GridSearchCV(LogisticRegression(multi_class='ovr', random_state=0), \n",
    "                  parameters).fit(X_train_sc, y_train)\n",
    "print('Best params:', gs.best_params_)\n",
    "\n",
    "print('\\n--- TRAINING SET ---')\n",
    "print('Accuracy:', accuracy_score(y_train, gs.predict(X_train_sc)))\n",
    "\n",
    "print('\\n--- TEST SET ---')\n",
    "print('Accuracy:', accuracy_score(y_test, gs.predict(X_test_sc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_train = sns.heatmap(confusion_matrix(y_train, gs.predict(X_train_sc), normalize=\"true\"), \n",
    "                            annot=True, fmt='.2%', cmap=\"PuBu\");\n",
    "heatmap_train.set_xlabel('Predicted label')\n",
    "heatmap_train.set_ylabel('True label')\n",
    "heatmap_train.set_title('Training set');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_test = sns.heatmap(confusion_matrix(y_test, gs.predict(X_test_sc), normalize=\"true\"), \n",
    "                           annot=True, fmt='.2%', cmap=\"PuBu\")\n",
    "heatmap_test.set_xlabel('Predicted label')\n",
    "heatmap_test.set_ylabel('True label')\n",
    "heatmap_test.set_title('Test set');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did you find? Should we apply more or less regularization to this problem to improve the accuracy? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multinomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the multinomial approach. This will be similar to what you did above, except now we'll set `multi_class='multinomial'` when we define our logistic regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train)\n",
    "X_test_sc = scaler.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression(multi_class='multinomial', random_state=0).fit(X_train_sc, y_train)\n",
    "\n",
    "print('--- TRAINING SET ---')\n",
    "print('Accuracy:', accuracy_score(y_train, clf.predict(X_train_sc)))\n",
    "\n",
    "print('\\n--- TEST SET ---')\n",
    "print('Accuracy:', accuracy_score(y_test, clf.predict(X_test_sc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's check out the confusion matrices for the training and test sets as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_train = # fill out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_test = # fill out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this compare to the OVR method?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run a grid search here too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'C':10**np.linspace(-2,2, 11)} # feel free to change this\n",
    "gs = GridSearchCV(LogisticRegression(multi_class='multinomial', random_state=0), \n",
    "                  parameters).fit(X_train_sc, y_train)\n",
    "print('Best params:', gs.best_params_)\n",
    "\n",
    "print('\\n--- TRAINING SET ---')\n",
    "print('Accuracy:', accuracy_score(y_train, gs.predict(X_train_sc)))\n",
    "\n",
    "print('\\n--- TEST SET ---')\n",
    "print('Accuracy:', accuracy_score(y_test, gs.predict(X_test_sc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_train = # fill out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_test = # fill out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "ctrl-shift-e"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
